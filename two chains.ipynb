{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently I was running 5k networks that had 50% conectivity with the inhibitory population. When two chains were embedded and simultaneously activated, the problem of global inhibition showed up. In fact, with a strong clustering within the gates and long chains, this problem also ocurrs when only one chain is embedded and activated. There seem to be a conflict between the global inhibition and the ability to create longer-lasting transients. \n",
    "\n",
    "I was wondering whether it is a general problem or a phenomenon that occured due to the actual tweaks I used in the network - 50% connectivity and recurrent connections deliberately stronger than the incoming ones. Thus, I revisited the old setup that I used - again 5k excitatory neurons, static synapses, incoming Poisson noise synapses stronger than the recurrent ones. It was the setup where I did not find any spontaneously generated sequences and only later I switched to the strengthened recurrent connection to finally get the sequences. This old network can be thought of as 'high acetylcholine state' where the input is stronger than recurrence so given enough input the sequences should fire.\n",
    "I wanted to check this network for the following reason: the connectivity is 5%, both with excitatory and inhibitory populations and the network seem to be strongly input driven. I wanted to check whether and how the global inhibition will affect the two chains activation. Also, since the recurrent connections are relatively weaker - will this prevent the activity from spreading and interferring with the chains?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Results </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it probably could have been predicted, similar problems occur in this setup. Below I put a multitude of plots with different parameters, which can probably be talked over in person. Overall, the findings are as follows:\n",
    "\n",
    "- even in 5% connectivity network where the external input is relatively stronger than the reccurent inputs the problem of a too strong wave of global inhibition prevails. \n",
    "\n",
    "- activating two chains at the same time in principle is possible, but the neurons belonging to different gates tend to fire together rather than in an orderly way due to a strong global excitation rather than a specific signal travelling along the chain\n",
    "\n",
    "- some instances of a network have a low success rate in signal transmission. Soma signal transmissions get disrupted as early as 3rd gate, some around 8th gate.\n",
    "\n",
    "\n",
    "<h2>Some thoughts </h2>\n",
    "Overall, the problem is stability. The networks seem to have some degree of variablity, as on some trials the signal transmission was successful, on some - the signal would die out in the middle of the chain. I was wondering whether some special conditions should be met while radnomly connecting the neurons. While we connect the neurons 'randomly' around the embedded chain, some patterns (clusters? chains? loops?) might occur and those can affect the signal transmission. But if such randomly occuring patterns indeed have some influence, I would need to find some group of networks with 100% succesful signal transmission and a group of networks (with the same parameters) with a large percentage of failed transmissions. Some simulations have 100% successful transmissions, some indeed have a large percentage of failed ones - but they still are capable of trasmitting the signal with no disturbance. Thus,for now I concluded that local connectivity is not that important, rather it's the current dynamics of a given gate that determines whether the signal will be transmitted further. A gate consisting of 100 excitatory neurons constantly receives external input and recurrent feedback. Then transiently it is bombarded with a strong input that is being transmitted to the next gate via the feedforward connectivity. The strength of this input was chosen and tested so that it's strong enough to activate the gate yet not too strong to cause multiple chain activation or destabilizing the whole net. Yet, on many trails this same input (volley of spikes) seems to be way too weak to activate the gate. The same chain while bombarded with the same volley gets activated 9 out of ten times. So what happens on that failed tenth trial? The connectivity remains the same, so it cannot be explained that some 'local circuitry' disrupted the expected randomness. I would say that it is the current state of the neurons - their subthreshold dynamics and recent (0-10ms) firing history. If a large number of neurons were strongly hyperpolarized, the volley will prove to be too weak to cause them to reach the threshold. This is actually a general rule of successful signal transmission - within a short time window a sufficient number of neurons has to fire in one gate to propagate the signal to the next gate. We can assume that on average the neurons remain at resting potential (-70mV) and it takes a certain number of (near-) simultaneous spikes to cause the neuron to reach the threshold (-57mV). Failed signal transmission refers to the case where too many neurons were well below -70mV and not enough spikes arrived to let the neuron reach the threshold. \n",
    "\n",
    "But then the question is - how to make such a setup stable and reliable? Or what is a range of parameters that keep the transmission stable? Or maybe some level of unreliability is somehow natural in brain and the way to circumvent it is some sort of redundancy? I started thinking of redundancy only recently, there is some evidence of it in the literature, but I haven't found anything strictly relevant. I was thinking that the whole idea of assemblies, distributed structures that hold or process information is somewhat reduntant from a start. I mean lots of neurons and synapses take part in some processing and swithing off a small subgroup of those might not affect the whole process. At the moment I work on 5k networks and the size of a gate is 100 excitatory and 25 inhibitory neurons - numbers chosen arbitrarily. As we don't have a reliable estimates of the real sizes of assemblies, we kind of use those toy examples to describe or predict what really might be happening in a full size assembly. But I think it is probably a common problem that some signal transmission/processing is unreliable. Whether some sort of redundancy is used to minimize the damage - I have no idea.\n",
    "\n",
    "Another thing is that again it is inhibition that seem to play a role. A scenario of a broken signal transmission is as follows - first gate receives the volley of spikes and send the signal to the second gate as well as every neuron excites 5% of other neurons in the background. Increased excitation is followed by increased inhibition and if it's too strong, neurons along the chain get too strongly hyperpolarized before the signal arrival and the signal is too weak to make them fire. One way to avoid this too early inhibition would be to use localised connectivity (as already proposed) but then I thought that maybe disinhibition could be used? I mean the deal now is to make sure the late gates don't receive too much inhibition too early - and the tau_inh=10ms so the actual timing of the signal is not that relevant as the result of inhibition prevails much longer than the excitation (1.5-2ms). Maybe in general the random, non-localised global inhibition can remain as it is, and only a small group of inhibitory neurons would be put locally along the chain as a scaffold and during the signal transmission would be inhibited by other inhibitory neurons so the gate neurons would be disinhibited and thus won't be hyperpolarized before the signal arrival. I was thinking how disinhibition relates to the delayed inhibition that we already have. I'm not sure whether it's correct thinking, but I see it that the delayed inhibition helps to prolong and control the time window when the neurons can integrate the excitatory signal to accumulate some transient and strong excitatory signal in a normally balanced network. Disinhibition, on the other hand, works before this time window. It is supposed to ensure that at the time of the signal integration, the neurons will be 'ready', 'not used' that is on average they won't be overly hyperpolarized by the increased global wave of inhibition. Of course the disinhibition would have to be controlled so it doesn't cause an excessive excitation. I would like to test it whether it's sensible to claim that disinhibition does something to neurons BEFORE they are about to integrate transient signal whereas the delayed inhibition is helping DURING this integration, all in a balanced network. Delayed inhibition works fine, but we already found some limitations so I want to add disinhibition, or maybe test the disinhibition on its own, without delayed inhibition?\n",
    "\n",
    "\n",
    "\n",
    "Generally, I was thinking of a following setup - at the moment I can generate networks that have less than 100% success rate in the signal transmission. Some are better, some are worse, but overall some numerical rate can be calculated. I was wondering to use it as a base and then test two possibilities - fully local inhibition and the other one - global inhibition plus some subset of disinhibitory neurons. If any of those solutions are good, the simulations on average should have a higher success rate.\n",
    "If one of the solutions works - it would be an interesting claim to make i guess. If both don't work - maybe more specific solutions would be tried until it works - and then we would have a case that to ensure a reliable transmission, the connectivity has to be fairly specific and in general the problem of unreliable transmission would prevail.\n",
    "Does it sound interesting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "img {\n",
    "    float: left; \n",
    "    width: 100%;\n",
    "}\n",
    "</style>\n",
    "<h2> loads of plots of activation of two chains. Some show a successful transmission, some show a failed one. To be discussed in person I guess </h2>\n",
    "<h3> no clustering, various lengths of chains </h3>\n",
    "\n",
    "<h4> chains - 8 and 8 long (default, so far I arbitrarily chose the chain to be 8-gate-long)</h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112822chain_8_8_0.png\"> </figure>\n",
    "That's the ideal situation - one chain is being activated 10 times, the other one - 5 times simultaneusly with the first one. the signal transmission is successful on all the trials. The two plots below show the example where the simultaneous transmission is successful, and it is the single signal transmission that fails on the way.\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121059chain_8_8_0.png\"> </figure>\n",
    "\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814114949chain_8_8_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 10 and 10 long </h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112841chain_10_10_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814115141chain_10_10_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121338chain_10_10_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 15 and 15 long </h4>\n",
    "\n",
    "in the plots below one thing becomes clear - when the signal propagation is along only one chain - the spikes look tilted - clear progression from one gate to another is visible. When the two chains are active - the gates seem to fire more simultaneously rather than in an orderly fashion\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121619chain_15_15_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814115304chain_15_15_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112856chain_15_15_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 8 and 5 long </h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112956chain_8_5_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814115327chain_8_5_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121542chain_8_5_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 8 and 3 long </h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112926chain_8_3_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814115219chain_8_3_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121431chain_8_3_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 8 and 2 long </h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814121527chain_8_2_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814115231chain_8_2_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814112915chain_8_2_0.png\"> </figure>\n",
    "\n",
    "<h4> chains - 8 and 1 long </h4>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814160329chain_8_0_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814153126chain_8_0_0.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814154613chain_8_0_0.png\"> </figure>\n",
    "\n",
    "\n",
    "<h3> 10% clustering, various lengths of chains </h3>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813180646chain_10_10_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813172632chain_10_10_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813172613chain_8_8_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813174841chain_8_3_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813174620chain_15_15_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813184118chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813185243chain_8_5_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813174416chain_8_8_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813174517chain_10_10_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813191404chain_8_3_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813180541chain_8_8_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813185101chain_8_3_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813173032chain_8_3_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813180931chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813181833chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813175510chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813180856chain_15_15_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814155401chain_8_0_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813174718chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814161129chain_8_0_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813182948chain_8_5_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813181048chain_8_3_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814162738chain_8_0_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813172653chain_15_15_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813172847chain_8_2_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813191717chain_8_5_10.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813182856chain_8_3_10.png\"> </figure>\n",
    "\n",
    "<h3> 20% clustering, various lengths of chains </h3>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813183350chain_10_10_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813192041chain_8_8_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813215854chain_8_3_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814161208chain_8_0_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813183408chain_15_15_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813192832chain_8_2_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813192210chain_15_15_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813192103chain_10_10_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813194009chain_8_5_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813221901chain_8_5_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813195039chain_8_2_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813215951chain_8_5_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813185647chain_8_8_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813190526chain_8_2_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814163036chain_8_0_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813221728chain_8_3_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813185731chain_10_10_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813183323chain_8_8_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813185819chain_15_15_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814155422chain_8_0_20.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813193942chain_8_3_20.png\"> </figure>\n",
    "\n",
    "<h3> 30% clustering, various lengths of chains </h3>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813194728chain_10_10_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813194716chain_8_8_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814002239chain_8_2_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814155442chain_8_0_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813235252chain_8_3_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813220627chain_8_8_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813235227chain_8_2_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814163313chain_8_0_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813222422chain_8_8_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813222521chain_10_10_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813220746chain_15_15_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813220701chain_10_10_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814000853chain_8_2_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814002551chain_8_3_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813235314chain_8_5_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814161241chain_8_0_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813222646chain_15_15_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814000814chain_8_3_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814002839chain_8_5_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150814001114chain_8_5_30.png\"> </figure>\n",
    "<figure><img src=\"files/oldext_2_all/cl20150813194741chain_15_15_30.png\"> </figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
